{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Process Book for CS109 Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Motivation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was pulled from the New York Times article API.  Using the API console (http://developer.nytimes.com/io-docs), we were able to inspect the type of results for a given query.  Originally, we decided to look at results using the 'fq=newsdesk:Health' option which would pull results under the Health topic section of the Times (approximately 680,000 documents).  However, after looking at the first 1000 results, we found that the documents pulled mainly consisted of videos, slideshows, and interactive features instead of articles.  We remedied this issue by instead using a query for the keyword 'Health' which searched all articles and their headlines for the word health.  Looking at the results from the API console showed that overall, using 'Health' as our query term instead of newsdesk:Health produced approiximately 40,000 more documents (720,000 total).\n",
    "\n",
    "The Times API has several limitnig factors when pulling data: 10,000 calls per day and a maximum of 100 pages per query.  To handle these limitations, our code pulled data by year and split each month into 3 parts (based on testing dates ranges for number of pages which would be pulled so the number of pages would be less than 100).  Year and count was entered manually to keep track of how many calls were being made and to break up the data calls for when errors occurred (such as timeouts, key errors, and date errors - when these errors were encountered the solution was appended to the code which resulted in the final version below).     \n",
    "\n",
    "In this code, we requested the json dictionaries, used the relevant information to create a dataframe with the date, id, document type (article, blog, video), newsdesk section and subsection, and text from the title, abstract, and first paragraph.  The data was saved in a csv file for that section - the files will be merged below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "count = 1460 #enter new count starting number\n",
    "year =  #enter year\n",
    "months = ['01', '01', '01', '02', '02', '02', '03', '03', '03', '04', '04', '04', '05', '05', '05', '06', '06', '06', '07', '07', '07', '08', '08', '08', '09', '09', '09', '10', '10', '10', '11', '11', '11', '12', '12', '12']\n",
    "startdays = ['01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21']\n",
    "enddays = ['10', '20', '31', '10', '20', '27', '10', '20', '31', '10', '20', '30', '10', '20', '31', '10', '20', '30', '10', '20', '31', '10', '20', '31', '10', '20', '30', '10', '20', '31', '10', '20', '30', '10', '20', '31']\n",
    "\n",
    "pcount = 0\n",
    "\n",
    "for d in range(36):\n",
    "    sdate = year + months[d] + startdays[d]\n",
    "    edate = year + months[d] + enddays[d]\n",
    "\n",
    "    docs=[]\n",
    "    #get 1st page and number of documents\n",
    "    url1 = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Health&page=1&begin_date={}&end_date={}&api-key=5cdab36b05348a4da2e74046dfb16a03:17:73541790\".format(sdate, edate) \n",
    "    lpage = requests.get(url1).json()['response']['meta']['hits']\n",
    "    \n",
    "    #calculate number of pages for call; if there are no hits skip to end\n",
    "    if lpage is not 0:\n",
    "        numpages = int(lpage/10 + 2) \n",
    "        pcount += numpages\n",
    "    \n",
    "        #get json files for first page\n",
    "        pagedoc1 = requests.get(url1).json()['response']['docs']\n",
    "        for j in range(0,len(pagedoc1)):\n",
    "            docs.append(pagedoc1[j])    \n",
    "    \n",
    "        #get json dictionaries for rest of the pages\n",
    "        for i in range(2, numpages): \n",
    "            url = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Health&page={}&begin_date={}&end_date={}&api-key=5cdab36b05348a4da2e74046dfb16a03:17:73541790\".format(i, sdate, edate)\n",
    "            pagedocs = requests.get(url).json()['response']['docs']\n",
    "            time.sleep(1)\n",
    "        \n",
    "            for j in range(0,len(pagedocs)):\n",
    "                docs.append(pagedocs[j])\n",
    "\n",
    "    #pull information from json file into dictionary        \n",
    "        docsinfo = []\n",
    "        for d in docs:\n",
    "            obs = {}\n",
    "            obs['id'] = d['_id']\n",
    "            obs['type'] = d['type_of_material']\n",
    "            obs['doctype'] = d['document_type']\n",
    "            obs['date'] = d['pub_date']\n",
    "            obs['news_desk'] = d['news_desk']\n",
    "            obs['section'] = d['section_name']\n",
    "            obs['subsection'] = d['subsection_name']\n",
    "            obs['abstract'] = d['abstract']\n",
    "            obs['paragraph'] = d['lead_paragraph']\n",
    "        \n",
    "            if d['headline'].get('main') is not None:\n",
    "                obs['headline'] = d['headline']['main']\n",
    "            elif d['headline'].get('name') is not None:\n",
    "                obs['headline'] = d['headline']['name']\n",
    "            else:\n",
    "                obs['headline'] = ' '\n",
    "    \n",
    "            if obs['date'] is not None:\n",
    "                obs['date'] = obs['date'][0:10]\n",
    "    \n",
    "            #Take out abstracts and lead paragraphs with none to join text\n",
    "            if obs['abstract'] is None:\n",
    "                a = ' '\n",
    "            else: \n",
    "                a = obs['abstract']\n",
    "            if obs['paragraph'] == 'TK TK TK' or obs['paragraph'] is None:\n",
    "                p = ' '\n",
    "            else:\n",
    "                p = obs['paragraph']\n",
    "    \n",
    "            text = [obs['headline'], p, a]\n",
    "            obs['text'] = \" \".join(text)\n",
    "    \n",
    "            docsinfo.append(obs)\n",
    "\n",
    "    #create dataframe from dictionary, make date column date type, and store in csv\n",
    "        docsdf = pd.DataFrame(docsinfo)\n",
    "        docsdf['date'] = pd.to_datetime(docsdf['date'])\n",
    "        docsdf.to_csv(\"data/docsdf-{}.csv\".format(count), encoding = 'utf-8') \n",
    "\n",
    "    print(count, pcount) \n",
    "    count += 1 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
