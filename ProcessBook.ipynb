{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Process Book for CS109 Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Motivation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project looks at trends in health topics over time.  The members of this project team are all public health students.  We were interested in how topics in health in a popular newspaper, The New York Times, have changed over time.  We decided to gain a deeper understanding of the Latent Dirilecht Allocation (LDA) method and use this topic modeling method to find the major topics in health over the past 5 decades (the period of time we were able to collect data for)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Which topics are persistent over time?  \n",
    " Which topics have a spike, when do they occur, and why did it happen?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was pulled from the New York Times article API.  Using the API console (http://developer.nytimes.com/io-docs), we were able to inspect the type of results for a given query.  Originally, we decided to look at results using the 'fq=newsdesk:Health' option which would pull results under the Health topic section of the Times (approximately 680,000 documents).  However, after looking at the first 1000 results, we found that the documents pulled mainly consisted of videos, slideshows, and interactive features instead of articles.  We remedied this issue by instead using a query for the keyword 'Health' which searched all articles and their headlines for the word health.  Looking at the results from the API console showed that overall, using 'Health' as our query term instead of newsdesk:Health produced approiximately 40,000 more documents (720,000 total).\n",
    "\n",
    "The Times API has several limitnig factors when pulling data: 10,000 calls per day and a maximum of 100 pages per query.  To handle these limitations, our code pulled data by year and split each month into 3 parts (based on testing dates ranges for number of pages which would be pulled so the number of pages would be less than 100).  Year and count was entered manually to keep track of how many calls were being made and to break up the data calls for when errors occurred (such as timeouts, key errors, and date errors - when these errors were encountered the solution was appended to the code which resulted in the final version below).     \n",
    "\n",
    "In this code, we requested the json dictionaries, used the relevant information to create a dataframe with the date, id, document type (article, blog, video), newsdesk section and subsection, and text from the title, abstract, and first paragraph.  The data was saved in a csv file for that section and tracked using an excel file - DateTracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "count = 1460 #enter new count starting number\n",
    "year =  #enter year\n",
    "months = ['01', '01', '01', '02', '02', '02', '03', '03', '03', '04', '04', '04', '05', '05', '05', '06', '06', '06', '07', '07', '07', '08', '08', '08', '09', '09', '09', '10', '10', '10', '11', '11', '11', '12', '12', '12']\n",
    "startdays = ['01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21']\n",
    "enddays = ['10', '20', '31', '10', '20', '27', '10', '20', '31', '10', '20', '30', '10', '20', '31', '10', '20', '30', '10', '20', '31', '10', '20', '31', '10', '20', '30', '10', '20', '31', '10', '20', '30', '10', '20', '31']\n",
    "\n",
    "pcount = 0\n",
    "\n",
    "for d in range(36):\n",
    "    sdate = year + months[d] + startdays[d]\n",
    "    edate = year + months[d] + enddays[d]\n",
    "\n",
    "    docs=[]\n",
    "    #get 1st page and number of documents\n",
    "    url1 = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Health&page=1&begin_date={}&end_date={}&api-key=5cdab36b05348a4da2e74046dfb16a03:17:73541790\".format(sdate, edate) \n",
    "    lpage = requests.get(url1).json()['response']['meta']['hits']\n",
    "    \n",
    "    #calculate number of pages for call; if there are no hits skip to end\n",
    "    if lpage is not 0:\n",
    "        numpages = int(lpage/10 + 2) \n",
    "        pcount += numpages\n",
    "    \n",
    "        #get json files for first page\n",
    "        pagedoc1 = requests.get(url1).json()['response']['docs']\n",
    "        for j in range(0,len(pagedoc1)):\n",
    "            docs.append(pagedoc1[j])    \n",
    "    \n",
    "        #get json dictionaries for rest of the pages\n",
    "        for i in range(2, numpages): \n",
    "            url = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Health&page={}&begin_date={}&end_date={}&api-key=5cdab36b05348a4da2e74046dfb16a03:17:73541790\".format(i, sdate, edate)\n",
    "            pagedocs = requests.get(url).json()['response']['docs']\n",
    "            time.sleep(1)\n",
    "        \n",
    "            for j in range(0,len(pagedocs)):\n",
    "                docs.append(pagedocs[j])\n",
    "\n",
    "    #pull information from json file into dictionary        \n",
    "        docsinfo = []\n",
    "        for d in docs:\n",
    "            obs = {}\n",
    "            obs['id'] = d['_id']\n",
    "            obs['type'] = d['type_of_material']\n",
    "            obs['doctype'] = d['document_type']\n",
    "            obs['date'] = d['pub_date']\n",
    "            obs['news_desk'] = d['news_desk']\n",
    "            obs['section'] = d['section_name']\n",
    "            obs['subsection'] = d['subsection_name']\n",
    "            obs['abstract'] = d['abstract']\n",
    "            obs['paragraph'] = d['lead_paragraph']\n",
    "        \n",
    "            if d['headline'].get('main') is not None:\n",
    "                obs['headline'] = d['headline']['main']\n",
    "            elif d['headline'].get('name') is not None:\n",
    "                obs['headline'] = d['headline']['name']\n",
    "            else:\n",
    "                obs['headline'] = ' '\n",
    "    \n",
    "            if obs['date'] is not None:\n",
    "                obs['date'] = obs['date'][0:10]\n",
    "    \n",
    "            #Take out abstracts and lead paragraphs with none to join text\n",
    "            if obs['abstract'] is None:\n",
    "                a = ' '\n",
    "            else: \n",
    "                a = obs['abstract']\n",
    "            if obs['paragraph'] == 'TK TK TK' or obs['paragraph'] is None:\n",
    "                p = ' '\n",
    "            else:\n",
    "                p = obs['paragraph']\n",
    "    \n",
    "            text = [obs['headline'], p, a]\n",
    "            obs['text'] = \" \".join(text)\n",
    "    \n",
    "            docsinfo.append(obs)\n",
    "\n",
    "    #create dataframe from dictionary, make date column date type, and store in csv\n",
    "        docsdf = pd.DataFrame(docsinfo)\n",
    "        docsdf['date'] = pd.to_datetime(docsdf['date'])\n",
    "        docsdf.to_csv(\"data/docsdf-{}.csv\".format(count), encoding = 'utf-8') \n",
    "\n",
    "    count += 1 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the files were concatenated into a single dataframe and saved to a csv file of our data which is located in our dropbox: https://www.dropbox.com/s/eedgwugamiw0zwd/total.csv?dl=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1966-2015\n",
    "#There were no documents for 08/11/1978 - 10/31/1978 (leading to gap in csv files)\n",
    "frames = []\n",
    "for i in range(1,1338) : \n",
    "    dfs = pd.read_csv(\"docsdf-\"+str(i)+\".csv\")\n",
    "    frames.append(dfs)\n",
    "for i in range(1346,1784) : \n",
    "    dfs = pd.read_csv(\"docsdf-\"+str(i)+\".csv\")\n",
    "    frames.append(dfs)\n",
    "totaldf = pd.concat(frames)\n",
    "totaldf['date'] = pd.to_datetime(totaldf['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totaldf.to_csv(\"total.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We looked at the types of documents to see if there were any trends and found that the majority of the documents were articles.  We are mainly interested in articles, and two of the document types are more recent types (multimedia and blogpost), we decided to only include articles in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to look at frequencies of document types\n",
    "sns.set(style=\"white\", context=\"talk\")\n",
    "ax = sns.barplot(x=('Article', 'Blogpost', 'Column', 'Multimedia', 'Recipe'), y=type_counts)\n",
    "ax.set(title=\"Document Type Frequencies\",ylim=(0,400000),yticks=[100000,200000,300000,400000])\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+0.2, height+10000, '%d'%height, fontsize=14)\n",
    "#ax.text(-0.2,380000, \"369,890\", fontsize=14)\n",
    "#ax.text(0.82,40000, \"31,369\", fontsize=14)\n",
    "sns.despine(bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only use documents that were specified as article will be used\n",
    "df = totaldf[totaldf['doctype'] == 'article']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used spark to clean and analyze our data since these processes are easily parallelized.  Spark was implemented using homebrew on a Mac. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/Applications/anaconda/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We began by cleaning our code in order to perform LDA.  We decided to use NLTK to process our text since it is the leading platform for natural language processing.  The text was tokenized, tagged for part of speech, made all lowercase, lemmatized, and the nouns were extracted.  \n",
    "\n",
    "*We considered issues with making words all lowercase due to some nouns such as AIDs and WHO but decided that not using .lower would potentially cause more problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer #token\n",
    "from nltk.corpus import stopwords #stopwords\n",
    "from nltk.stem import WordNetLemmatizer #lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The necessary NLTK packages need to be downloaded to implement some of the functions\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The method information needed to clean the data\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "stops = stopwords.words('english')\n",
    "stops.append(u'health') #Include health in our stopwords since it will be in most if not all of our documents\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean the text for each document to extract the nouns \n",
    "#Cleaning includes lemmatization and removal of stopwords\n",
    "\n",
    "def get_parts(thetext):\n",
    "    nouns=[]\n",
    "    tokens = TreebankWordTokenizer().tokenize(thetext)\n",
    "    tagged = nltk.pos_tag(tokens) # a list of tuples\n",
    "    for tup in tagged : \n",
    "        w, tag = tup  \n",
    "        print w, tag\n",
    "        if tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            word = wnl.lemmatize(w.lower())\n",
    "            print word\n",
    "            if word[-1] in punctuation : \n",
    "                word = word[:-1]\n",
    "            if word in stops or word in punctuation or len(word)==1 :\n",
    "                continue\n",
    "            nouns.append(word)\n",
    "    nouns2=[]\n",
    "    for n in nouns:\n",
    "        if len(n)!=0:\n",
    "            nouns2.append(n)\n",
    "    return nouns2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "parseout = []\n",
    "for index, row in df.iterrows() : \n",
    "    parseout.append(get_parts(row.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "document = [list(itertools.chain.from_iterable(p)) for p in parseout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get a list of words in each document compiled into a list \n",
    "ldadatardd=sc.parallelize(parseout).flatMap(lambda l: l)\n",
    "ldadatardd.cache()\n",
    "ldadatardd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get a wordcount and a list of tuples containing a word from our data and an index for that word\n",
    "\n",
    "wordcount = (ldadatardd.flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b))\n",
    "\n",
    "vocabtups = (wordcount.map(lambda (x,y): x)\n",
    "             .zipWithIndex()\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab=vocabtups.collectAsMap()\n",
    "id2word=vocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get a count of the words in a document\n",
    "from collections import defaultdict\n",
    "def get_count(s) : \n",
    "    d = defaultdict(int)\n",
    "    for w in s : \n",
    "        if vocab.has_key(w) :\n",
    "            i = vocab[w]\n",
    "            d[i] += 1\n",
    "    return d.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Collect the words from each document in our data into a corpus\n",
    "documents = ldadatardd.map(get_count)\n",
    "corpus=documents.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda2015 = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=50, update_every=1, chunksize=10000, passes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Decisions when performing LDA \n",
    " \n",
    " We decided to consider topics by decade since we know health topics will change over time and a decade is a reasonable time frame **Need better justification**.  *(If we had more time for this project, we would like to consider other time frames).\n",
    " \n",
    " We used the a document as our cluster size when performing LDA (instead of sentence) under the assumption that each article has a main topic.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
