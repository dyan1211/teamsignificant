{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Process Book for CS109 Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Motivation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project looks at trends in health topics over time.  The members of this project team are all public health students.  We were interested in how topics in health in a popular newspaper, The New York Times, have changed over time.  We decided to gain a deeper understanding of the Latent Dirilecht Allocation (LDA) method and use this topic modeling method to find the major topics in health over the past 5 decades (the period of time we were able to collect data for)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Which topics are persistent over time?  \n",
    " Which topics have a spike, when do they occur, and why did it happen? \n",
    " Are there any surprising topics?\n",
    " \n",
    " With regards to LDA:\n",
    " Should we use Pattern (as in Hw5) or is there a better option for our data?\n",
    " What hyperparameters should we choose?\n",
    " \n",
    " Are there further analyses we can consider based on our findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was pulled from the New York Times article API.  Using the API console (http://developer.nytimes.com/io-docs), we were able to inspect the type of results for a given query.  Originally, we decided to look at results using the 'fq=newsdesk:Health' option which would pull results under the Health topic section of the Times (approximately 680,000 documents).  However, after looking at the first 1000 results, we found that the documents pulled mainly consisted of videos, slideshows, and interactive features instead of articles.  We remedied this issue by instead using a query for the keyword 'Health' which searched all articles and their headlines for the word health.  Looking at the results from the API console showed that overall, using 'Health' as our query term instead of newsdesk:Health produced approiximately 40,000 more documents (720,000 total).\n",
    "\n",
    "The Times API has several limitnig factors when pulling data: 10,000 calls per day and a maximum of 100 pages per query.  To handle these limitations, our code pulled data by year and split each month into 3 parts (based on testing dates ranges for number of pages which would be pulled so the number of pages would be less than 100).  Year and count was entered manually to keep track of how many calls were being made and to break up the data calls for when errors occurred (such as timeouts, key errors, and date errors - when these errors were encountered the solution was appended to the code which resulted in the final version below).     \n",
    "\n",
    "In this code, we requested the json dictionaries, used the relevant information to create a dataframe with the date, id, document type (article, blog, video), newsdesk section and subsection, and text from the title, abstract, and first paragraph.  The data was saved in a csv file for that section and tracked using an excel file - DateTracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "count = 1460 #enter new count starting number\n",
    "year =  #enter year\n",
    "months = ['01', '01', '01', '02', '02', '02', '03', '03', '03', '04', '04', '04', '05', '05', '05', '06', '06', '06', '07', '07', '07', '08', '08', '08', '09', '09', '09', '10', '10', '10', '11', '11', '11', '12', '12', '12']\n",
    "startdays = ['01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21', '01', '11', '21']\n",
    "enddays = ['10', '20', '31', '10', '20', '27', '10', '20', '31', '10', '20', '30', '10', '20', '31', '10', '20', '30', '10', '20', '31', '10', '20', '31', '10', '20', '30', '10', '20', '31', '10', '20', '30', '10', '20', '31']\n",
    "\n",
    "pcount = 0\n",
    "\n",
    "for d in range(36):\n",
    "    sdate = year + months[d] + startdays[d]\n",
    "    edate = year + months[d] + enddays[d]\n",
    "\n",
    "    docs=[]\n",
    "    #get 1st page and number of documents\n",
    "    url1 = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Health&page=1&begin_date={}&end_date={}&api-key=5cdab36b05348a4da2e74046dfb16a03:17:73541790\".format(sdate, edate) \n",
    "    lpage = requests.get(url1).json()['response']['meta']['hits']\n",
    "    \n",
    "    #calculate number of pages for call; if there are no hits skip to end\n",
    "    if lpage is not 0:\n",
    "        numpages = int(lpage/10 + 2) \n",
    "        pcount += numpages\n",
    "    \n",
    "        #get json files for first page\n",
    "        pagedoc1 = requests.get(url1).json()['response']['docs']\n",
    "        for j in range(0,len(pagedoc1)):\n",
    "            docs.append(pagedoc1[j])    \n",
    "    \n",
    "        #get json dictionaries for rest of the pages\n",
    "        for i in range(2, numpages): \n",
    "            url = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Health&page={}&begin_date={}&end_date={}&api-key=5cdab36b05348a4da2e74046dfb16a03:17:73541790\".format(i, sdate, edate)\n",
    "            pagedocs = requests.get(url).json()['response']['docs']\n",
    "            time.sleep(1)\n",
    "        \n",
    "            for j in range(0,len(pagedocs)):\n",
    "                docs.append(pagedocs[j])\n",
    "\n",
    "    #pull information from json file into dictionary        \n",
    "        docsinfo = []\n",
    "        for d in docs:\n",
    "            obs = {}\n",
    "            obs['id'] = d['_id']\n",
    "            obs['type'] = d['type_of_material']\n",
    "            obs['doctype'] = d['document_type']\n",
    "            obs['date'] = d['pub_date']\n",
    "            obs['news_desk'] = d['news_desk']\n",
    "            obs['section'] = d['section_name']\n",
    "            obs['subsection'] = d['subsection_name']\n",
    "            obs['abstract'] = d['abstract']\n",
    "            obs['paragraph'] = d['lead_paragraph']\n",
    "        \n",
    "            if d['headline'].get('main') is not None:\n",
    "                obs['headline'] = d['headline']['main']\n",
    "            elif d['headline'].get('name') is not None:\n",
    "                obs['headline'] = d['headline']['name']\n",
    "            else:\n",
    "                obs['headline'] = ' '\n",
    "    \n",
    "            if obs['date'] is not None:\n",
    "                obs['date'] = obs['date'][0:10]\n",
    "    \n",
    "            #Take out abstracts and lead paragraphs with none to join text\n",
    "            if obs['abstract'] is None:\n",
    "                a = ' '\n",
    "            else: \n",
    "                a = obs['abstract']\n",
    "            if obs['paragraph'] == 'TK TK TK' or obs['paragraph'] is None:\n",
    "                p = ' '\n",
    "            else:\n",
    "                p = obs['paragraph']\n",
    "    \n",
    "            text = [obs['headline'], p, a]\n",
    "            obs['text'] = \" \".join(text)\n",
    "    \n",
    "            docsinfo.append(obs)\n",
    "\n",
    "    #create dataframe from dictionary, make date column date type, and store in csv\n",
    "        docsdf = pd.DataFrame(docsinfo)\n",
    "        docsdf['date'] = pd.to_datetime(docsdf['date'])\n",
    "        docsdf.to_csv(\"data/docsdf-{}.csv\".format(count), encoding = 'utf-8') \n",
    "\n",
    "    count += 1 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the files were concatenated into a single dataframe and saved to a csv file of our data which is located in our dropbox: https://www.dropbox.com/s/eedgwugamiw0zwd/total.csv?dl=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1966-2015\n",
    "#There were no documents for 08/11/1978 - 10/31/1978 (leading to gap in csv files)\n",
    "frames = []\n",
    "for i in range(1,1338) : \n",
    "    dfs = pd.read_csv(\"docsdf-\"+str(i)+\".csv\")\n",
    "    frames.append(dfs)\n",
    "for i in range(1346,1784) : \n",
    "    dfs = pd.read_csv(\"docsdf-\"+str(i)+\".csv\")\n",
    "    frames.append(dfs)\n",
    "totaldf = pd.concat(frames)\n",
    "totaldf['date'] = pd.to_datetime(totaldf['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totaldf.to_csv(\"total.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We looked at the types of documents to see if there were any trends and found that the majority of the documents were articles.  We are mainly interested in articles, and two of the document types are more recent types (multimedia and blogpost), we decided to only include articles in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to look at frequencies of document types\n",
    "sns.set(style=\"white\", context=\"talk\")\n",
    "ax = sns.barplot(x=('Article', 'Blogpost', 'Column', 'Multimedia', 'Recipe'), y=type_counts)\n",
    "ax.set(title=\"Document Type Frequencies\",ylim=(0,400000),yticks=[100000,200000,300000,400000])\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+0.2, height+10000, '%d'%height, fontsize=14)\n",
    "#ax.text(-0.2,380000, \"369,890\", fontsize=14)\n",
    "#ax.text(0.82,40000, \"31,369\", fontsize=14)\n",
    "sns.despine(bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i.imgur.com/qLMXD3e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only use documents that were specified as article will be used\n",
    "df = totaldf[totaldf['doctype'] == 'article']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting our data over time and checking days which had highest number of articles, we realized there were a number of duplicate documents which had pieces of another documents (such as a shorted title or only the first few sentences of a paragraph). We utilized the drop_duplicates function to by creating a new column which contained the first 50 characters of the paragraph column in order to capture the matching sections of the two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bar graph of number of documents per year before dropping duplicates (spike in 1980 and 2014)\n",
    "years = df.year.value_counts().sort_index()\n",
    "years.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgur.com/rVyNJx1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check dates with most articles \n",
    "days = articledf.date.value_counts()\n",
    "days[:20].plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgur.com/caAkxtg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove duplicates\n",
    "df['dupcheck'] = df['paragraph'].str[0:50]\n",
    "df = df.drop_duplicates('dupcheck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bar graph of number of documents per year after dropping duplicates (smoothed out 1980, though still 2014 spike )\n",
    "years = df.year.value_counts().sort_index()\n",
    "years.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgur.com/dHtCfzN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph looks much better though there is still the 2014 spike.  Using the API console through NYT, I check the total number of articles in 2014 compared to 2013 (not just 'Health' articles):\n",
    "\n",
    "Total # of articles produced by NYT in 2013 = 117593\n",
    "\n",
    "Total # of articles produced by NYT in 2014 = 308867\n",
    "\n",
    "So the spike is not specific to our search, though we do not have a concrete explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of Time Periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving forward, we made the decision that in order to parse out clear and specific topics we should perform LDA on subsets of the data.  By using the entire dataset, we decided the topics would likely end up being too general.  We decided to use 5 year periods, large enough have data to run LDA on and to capture topics which would have a trend in time, but small enough that some major events would show up. \n",
    "\n",
    "The code for splitting the df was done in >>>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used spark to clean and analyze our data since these processes are easily parallelized.  Spark was implemented using homebrew on a Mac. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/Applications/anaconda/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We began by cleaning our code in order to perform LDA.  We decided to use NLTK to process our text since it is the leading platform for natural language processing.  However, we found that NLTK takes an extremely long time to process our large data set.  Instead, we utilized TextBlob which works off of both NLTK and Pattern platforms.  TextBlob also has the advantage of implementing the Averaged Perceptron algorithm which has been shown to be faster and more accurate than NLTK and Pattern (http://stevenloria.com/tutorial-state-of-the-art-part-of-speech-tagging-in-textblob/).\n",
    "\n",
    "The text was tokenized, tagged for part of speech, made all lowercase, lemmatized, and the nouns were extracted.  \n",
    "\n",
    "We considered issues with making words all lowercase due to some nouns such as AIDs and WHO but decided that not using .lower would potentially cause more problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob as tb\n",
    "from textblob_aptagger import PerceptronTagger\n",
    "from textblob import Blobber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords #stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The necessary NLTK packages need to be downloaded to implement some of the functions. RUN ONLY ONCE\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up Averaged Perceptron Tagger for TextBlob\n",
    "tb = Blobber(pos_tagger=PerceptronTagger())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean the text for each document to extract the nouns \n",
    "#Cleaning includes lemmatization and removal of stopwords\n",
    "\n",
    "def get_parts(thetext):\n",
    "    nouns=[]\n",
    "    tagged = tb(thetext).tags # a list of tuples\n",
    "    for tup in tagged:\n",
    "        w, tag = tup  \n",
    "        if tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            word = w.lemmatize().lower()\n",
    "            if word[-1] in punctuation : \n",
    "                word = word[:-1]\n",
    "            if word in stops or word in punctuation or len(word)==1 :\n",
    "                continue\n",
    "            nouns.append(word)\n",
    "    nouns2=[]\n",
    "    for n in nouns:\n",
    "        if len(n)!=0:\n",
    "            nouns2.append(n)\n",
    "    return nouns2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse out the nouns and organize in a list for each document.  We decided to parse on the document level (instead of the sentence level) under the assumption that each document would be have a topic instead of each sentence.  \n",
    "\n",
    "*This code was run separately for each 5 year period - all example output is for 1966"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "parseout = []\n",
    "for index, row in df.iterrows() : \n",
    "    parseout.append(get_parts(row.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving parseout to txt file\n",
    "with open(\"parseout.txt\", 'w') as f:\n",
    "    json.dump(parseout,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the output for each 5-year time period, we were able to create word clouds to get an idea of the words in our corpus.  Not surprisingly, the words 'New' and 'Times' are largest in most of these clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the mask image taken from http://static01.nyt.com/images/icons/t_logo_291_black.png\n",
    "nyt_mask = np.array(Image.open(\"~/nyt_mask_2.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=2000, mask=nyt_mask, stopwords=STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create single string of all text for each parseout list\n",
    "import itertools\n",
    "document1966 = list(itertools.chain.from_iterable(parseout1966))\n",
    "document1966 = ' '.join(document1966)\n",
    "wc.generate(document1966)  #generate wordcloud\n",
    "\n",
    "# show\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i.imgur.com/m7nfPPr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA \n",
    "\n",
    "Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, matutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(documents)\n",
    "dictionary.filter_extremes(no_below=5,no_above=0.75,keep_n=100000)\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(document) for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used KL Divergence to decide number of topics separately for each 5-year time period separately\n",
    "Code for this section from: http://blog.cigrainger.com/2014/07/lda-number.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sym_kl(p,q):\n",
    "    return np.sum([stats.entropy(p,q),stats.entropy(q,p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arun(corpus,dictionary,min_topics,max_topics,step):\n",
    "    kl = []\n",
    "    for i in range(min_topics,max_topics,step):\n",
    "        lda = models.ldamodel.LdaModel(corpus=corpus,\n",
    "            id2word=dictionary,chunksize=340,num_topics=i)  #we decided on a chunksize of approximately .01 of number of documents\n",
    "        m1 = lda.expElogbeta\n",
    "        U,cm1,V = np.linalg.svd(m1)\n",
    "        #Document-topic matrix\n",
    "        lda_topics = lda[corpus]\n",
    "        m2 = matutils.corpus2dense(lda_topics, lda.num_topics).transpose()\n",
    "        cm2 = l.dot(m2)\n",
    "        cm2 = cm2 + 0.0001\n",
    "        cm2norm = np.linalg.norm(l)\n",
    "        cm2 = cm2/cm2norm\n",
    "        kl.append(sym_kl(cm1,cm2))\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = np.array([sum(cnt for _, cnt in doc) for doc in corpus])\n",
    "kl = arun(corpus,dictionary,min_topics=1,max_topics=100,step=5)  #searched best number of topics between 1 and 100 by 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot kl divergence against number of topics\n",
    "x = np.arange(1,100,5)\n",
    "plt.plot(x, kl)\n",
    "plt.ylabel('Symmetric KL Divergence')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.savefig('kldiv1986.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i.imgur.com/9aRvg9m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA: In a 5-year period, chunksize determined by 0.01 of number of documents, number of topics decided from KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary,chunksize=220, num_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda.print_topics(num_topics=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new data frame \n",
    "\n",
    "We appended to each 5-year dataframe a column for each topic.  The data in these columns was the percent of the document attributed to that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking topics - we looked at the top 10-20 articles with the highest percent for a topic to see the specific articles which fell under the topic (useful for unclear topics and identifying specific events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the top 20 articles with the highest percent for a topic to \n",
    "top20 = [list(fulldf['Topic1']).index(p) for p in heapq.nlargest(20, list(fulldf['Topic1']))]\n",
    "list(fulldf['text'][top20])[0:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
